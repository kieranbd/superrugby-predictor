{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "a1jqgM4V-5HE"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pd.set_option('display.max_columns', 100)\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "from xgboost import XGBRegressor\n",
    "import shap\n",
    "shap.initjs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8gZRyWwC_8Rz"
   },
   "outputs": [],
   "source": [
    "# read csv with specified date column, drop unnecessary columns\n",
    "df = pd.read_csv(\n",
    "    \"./data/super_rugby_oddsportal.csv\", parse_dates =[\"Date\"]\n",
    ").drop(\n",
    "    ['Play-off Game?', 'Bookmakers Surveyed'], axis=1  # drop unnecessary columns\n",
    ")\n",
    "\n",
    "# ** is this necessary ?\n",
    "spare = pd.read_csv(\"./data/super_rugby_oddsportal.csv\").drop('Play-off Game?', axis=1).dropna()\n",
    "\n",
    "# number of upcoming fixtures\n",
    "n_matches = len(df[(df['Home Score']==0) & (df['Away Score']==0)])\n",
    "\n",
    "df.head(n_matches)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 1. Feature engineering\n",
    "### Numeric columns\n",
    "  - `home_margin` = `home_score` - `away_score`: score differential w.r.t. home team\n",
    "  - `home_win` = `home_score` > `away_score`: did the home team win (1/0)?\n",
    "    - We will use this in the next block to create a moving window of home / away team win rates\n",
    "  - `home_odds` = `home_score` - `away_score`: transformed bookmakers' odds into form pr(home team win)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# standardise naming format\n",
    "df.columns = [x.lower().replace(' ', '_') for x in df.columns]\n",
    "\n",
    "# extract year\n",
    "df['year'] = df['date'].apply(lambda x: x.year)\n",
    "\n",
    "# add home margin column\n",
    "df['home_margin'] = df['home_score'] - df['away_score']\n",
    "# add home win column\n",
    "df['home_win'] = (df['home_score'] > df['away_score']).astype('int')\n",
    "\n",
    "# aggregate odds into single probability variable\n",
    "df['home_odds'] = df['away_odds'] / (df['home_odds'] + df['away_odds'])\n",
    "df.drop(['draw_odds', 'away_odds'], axis=1, inplace=True)\n",
    "\n",
    "df.head(15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Window functions\n",
    "  - avg points scored by home side in last n games\n",
    "  - avg points conceded by home side in last n games\n",
    "  - avg points scored by away side in last n games\n",
    "  - avg points conceded by away side in last n games"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.assign(\n",
    "    # average odds for home_team in previous n home fixtures\n",
    "    avg_hm_odd_5=df.groupby('home_team')['home_odds'].rolling(5).mean().shift(-5).reset_index(0, drop=True),\n",
    "    avg_hm_odd_10=df.groupby('home_team')['home_odds'].rolling(10).mean().shift(-10).reset_index(0, drop=True),\n",
    "    avg_hm_odd_20=df.groupby('home_team')['home_odds'].rolling(20).mean().shift(-20).reset_index(0, drop=True),\n",
    "    # average odds for away team in previous n away fixtures\n",
    "    avg_aw_odd_5= 1-df.groupby('away_team')['home_odds'].rolling(5).mean().shift(-5).reset_index(0, drop=True),\n",
    "    avg_aw_odd_10= 1-df.groupby('away_team')['home_odds'].rolling(10).mean().shift(-10).reset_index(0, drop=True),\n",
    "    avg_aw_odd_20= 1-df.groupby('away_team')['home_odds'].rolling(20).mean().shift(-20).reset_index(0, drop=True),\n",
    "    # home team win rate previous n\n",
    "    hm_wr_5=df.groupby('home_team')['home_win'].rolling(5).sum().shift(-5).reset_index(0, drop=True)/5,\n",
    "    hm_wr_10=df.groupby('home_team')['home_win'].rolling(10).sum().shift(-10).reset_index(0, drop=True)/10,\n",
    "    hm_wr_20=df.groupby('home_team')['home_win'].rolling(20).sum().shift(-20).reset_index(0, drop=True)/20,\n",
    "    # away team win rate previous n\n",
    "    aw_wr_5= 1-df.groupby('away_team')['home_win'].rolling(5).sum().shift(-5).reset_index(0, drop=True)/5,\n",
    "    aw_wr_10= 1-df.groupby('away_team')['home_win'].rolling(10).sum().shift(-10).reset_index(0, drop=True)/10,\n",
    "    aw_wr_20= 1-df.groupby('away_team')['home_win'].rolling(20).sum().shift(-20).reset_index(0, drop=True)/20,\n",
    "    # average margin by home_team in previous n home fixtures\n",
    "    avg_hm_marg_5=df.groupby('home_team')['home_margin'].rolling(5).mean().shift(-5).reset_index(0, drop=True),\n",
    "    avg_hm_marg_10=df.groupby('home_team')['home_margin'].rolling(10).mean().shift(-10).reset_index(0, drop=True),\n",
    "    avg_hm_marg_20=df.groupby('home_team')['home_margin'].rolling(20).mean().shift(-20).reset_index(0, drop=True)\n",
    ")\n",
    "\n",
    "df.drop(['date', 'home_score', 'away_score', 'home_win'], axis=1, inplace=True)\n",
    "\n",
    "df = df.dropna().reset_index(drop=True)\n",
    "\n",
    "df.head(n_matches)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Categorical columns\n",
    "  - home team / country\n",
    "  - away team / country\n",
    "  - year\n",
    "  \n",
    "Drop `country = Argentina` and `country = Japan` and all teams not currently in competition.\n",
    "  \n",
    "> **Test**: One-hot encoding vs label encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tAoc1xkpI9b-"
   },
   "outputs": [],
   "source": [
    "countries = {'Crusaders': 'NZ',\n",
    "             'Chiefs': 'NZ',\n",
    "             'Blues': 'NZ',\n",
    "             'Hurricanes': 'NZ',\n",
    "             'Highlanders': 'NZ',\n",
    "             'Bulls': 'SA',\n",
    "             'Cheetahs': 'SA',\n",
    "             'Kings': 'SA',\n",
    "             'Lions': 'SA',\n",
    "             'Sharks': 'SA',\n",
    "             'Stormers': 'SA',\n",
    "             'Brumbies': 'AUS',\n",
    "             'Force': 'AUS',\n",
    "             'Rebels': 'AUS',\n",
    "             'Reds': 'AUS',\n",
    "             'Waratahs': 'AUS',\n",
    "             'Jaguares': 'ARG',\n",
    "             'Sunwolves': 'JPN'}\n",
    "\n",
    "# add nationalities\n",
    "df['home_country'] = df['home_team'].replace(countries)\n",
    "df['away_country'] = df['away_team'].replace(countries)\n",
    "\n",
    "# convert to pandas category dtypes\n",
    "df[\n",
    "    ['home_team', 'away_team', 'home_country', 'away_country']\n",
    "] = df[\n",
    "    ['home_team', 'away_team', 'home_country', 'away_country']\n",
    "].astype('category')\n",
    "\n",
    "# one-hot encode nationalities\n",
    "df = pd.get_dummies(df, prefix='home_country', columns=['home_country'])\n",
    "df = pd.get_dummies(df, prefix='away_country', columns=['away_country'])\n",
    "    \n",
    "# one-hot encode team names\n",
    "df = pd.get_dummies(df, prefix='home_team', columns=['home_team'])\n",
    "df = pd.get_dummies(df, prefix='away_team', columns=['away_team'])\n",
    "\n",
    "# drop irrelevent columns\n",
    "df.drop(['home_country_ARG', 'home_country_JPN', 'away_country_ARG', 'away_country_JPN',\n",
    "         'home_team_Cheetahs', 'away_team_Cheetahs', 'home_team_Kings', \n",
    "         'away_team_Kings', 'home_team_Force', 'away_team_Force'], \n",
    "        axis=1,\n",
    "        inplace=True)\n",
    "\n",
    "df.head(n_matches)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Nip3HfwNI9cS"
   },
   "outputs": [],
   "source": [
    "X = df[n_matches:].drop(['home_margin'], axis=1)\n",
    "y = df[n_matches:].home_margin.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Nip3HfwNI9cS"
   },
   "outputs": [],
   "source": [
    "# upcoming week's fixtures\n",
    "X_temp = df[:n_matches].drop(['home_margin'], axis=1)\n",
    "y_temp = df[:n_matches].home_margin.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 2. Train model\n",
    "  - To do: train neural network using embeddings for teams instead of one-hot encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fuRKmrGTTlwV"
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.1, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = XGBRegressor(max_depth=1, learning_rate=.1, n_estimators=100, n_jobs=-1, min_child_weight=5, subsample=.45, random_state=0)\n",
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1090,
     "status": "ok",
     "timestamp": 1551945723795,
     "user": {
      "displayName": "Kieran Donnelly",
      "photoUrl": "https://lh5.googleusercontent.com/-qOCnWQmSPhU/AAAAAAAAAAI/AAAAAAAA3YM/gLNo024IFgk/s64/photo.jpg",
      "userId": "08966570757713512847"
     },
     "user_tz": -120
    },
    "id": "OOvdi6F0WJgZ",
    "outputId": "860829a9-b6e7-45b1-9d23-4ddabf76ddc7"
   },
   "outputs": [],
   "source": [
    "# training data\n",
    "print(f'Train RMSE: {np.sqrt(mean_squared_error(model.predict(X_train), y_train)):.3f}')\n",
    "print(f'Train MAE: {mean_absolute_error(y_train, model.predict(X_train)):.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1090,
     "status": "ok",
     "timestamp": 1551945723795,
     "user": {
      "displayName": "Kieran Donnelly",
      "photoUrl": "https://lh5.googleusercontent.com/-qOCnWQmSPhU/AAAAAAAAAAI/AAAAAAAA3YM/gLNo024IFgk/s64/photo.jpg",
      "userId": "08966570757713512847"
     },
     "user_tz": -120
    },
    "id": "OOvdi6F0WJgZ",
    "outputId": "860829a9-b6e7-45b1-9d23-4ddabf76ddc7"
   },
   "outputs": [],
   "source": [
    "# test data\n",
    "print(f'Test RMSE: {np.sqrt(mean_squared_error(model.predict(X_test), y_test)):.3f}')\n",
    "print(f'Test MAE: {mean_absolute_error(y_test, model.predict(X_test)):.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train on all data (if not overfitting)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# include feature standardizer for numeric columns\n",
    "\n",
    "# from sklearn.preprocessing import StandardScaler\n",
    "# from sklearn.compose import ColumnTransformer\n",
    "# from sklearn.pipeline import Pipeline\n",
    "\n",
    "# # standardize first two columns\n",
    "# ct = ColumnTransformer(\n",
    "#     [('scaler', StandardScaler(), [0, 1, -1])]\n",
    "# )\n",
    "\n",
    "# xgb = XGBRegressor(max_depth=1, learning_rate=.1, n_estimators=100, n_jobs=-1, min_child_weight=5, subsample=.45, random_state=0)\n",
    "\n",
    "# model = Pipeline(\n",
    "#     [\n",
    "#         ('transformer', ct),\n",
    "#         ('regressor', xgb)        \n",
    "#     ]\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(X, y)\n",
    "\n",
    "print(f'RMSE: {np.sqrt(mean_squared_error(model.predict(X), y)):.3f}')\n",
    "print(f'MAE: {mean_absolute_error(y, model.predict(X)):.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SHAP values\n",
    "[Here is a nice notebook tutorial](https://slundberg.github.io/shap/notebooks/Census%20income%20classification%20with%20XGBoost.html)  for working with SHAP values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "explainer = shap.TreeExplainer(model)\n",
    "shap_values = explainer.shap_values(X)\n",
    "shap.summary_plot(shap_values, X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.summary_plot(shap_values, X, plot_type=\"bar\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.force_plot(explainer.expected_value, shap_values[:1000,:], X.iloc[:1000,:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gLHinnhzI9c6"
   },
   "source": [
    "## 3. Make predictions for upcoming week"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 136
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 2812,
     "status": "ok",
     "timestamp": 1551946869126,
     "user": {
      "displayName": "Kieran Donnelly",
      "photoUrl": "https://lh5.googleusercontent.com/-qOCnWQmSPhU/AAAAAAAAAAI/AAAAAAAA3YM/gLNo024IFgk/s64/photo.jpg",
      "userId": "08966570757713512847"
     },
     "user_tz": -120
    },
    "id": "W2L2WMCSI9dE",
    "outputId": "9161e521-5295-4c74-d4fa-34501b61c823"
   },
   "outputs": [],
   "source": [
    "# predict for upcoming week\n",
    "np.vstack((model.predict(X_temp), spare['Home Team'][:n_matches], spare['Away Team'][:n_matches])).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Oaf4jO-7Lzd0"
   },
   "outputs": [],
   "source": [
    "temp = np.vstack((model.predict(X_temp), spare['Home Team'][:n_matches], spare['Away Team'][:n_matches])).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "smWtk-ptM1P3"
   },
   "outputs": [],
   "source": [
    "preds_df = pd.DataFrame(temp, columns=[\"Home_Margin\", \"Home_Team\", \"Away_Team\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 266
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 807,
     "status": "ok",
     "timestamp": 1551946869131,
     "user": {
      "displayName": "Kieran Donnelly",
      "photoUrl": "https://lh5.googleusercontent.com/-qOCnWQmSPhU/AAAAAAAAAAI/AAAAAAAA3YM/gLNo024IFgk/s64/photo.jpg",
      "userId": "08966570757713512847"
     },
     "user_tz": -120
    },
    "id": "Q1sEAiR-M5Va",
    "outputId": "f63b4b87-ac17-4a40-c8d7-2f24f2442ded"
   },
   "outputs": [],
   "source": [
    "preds_df[\"Home_Margin\"] = preds_df.Home_Margin.apply(lambda x: int(np.round(x, 0)))\n",
    "preds_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explain single prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MATCH_NO = 6\n",
    "\n",
    "print(f'Explaining result of  {preds_df.iloc[MATCH_NO,1]} vs {preds_df.iloc[MATCH_NO,2]}:\\n')\n",
    "shap_values = explainer.shap_values(X_temp)\n",
    "shap.force_plot(explainer.expected_value, shap_values[MATCH_NO,:], X_temp.iloc[MATCH_NO,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Super_Rugby_Predictor_James_22_Feb.ipynb",
   "provenance": [],
   "toc_visible": true,
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
